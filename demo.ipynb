{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from typing import List\n",
    "from config import FT_NET_CFG, SHAPE_EMBEDDING_CFG\n",
    "from src.models.baseline import LitModule\n",
    "from src.datasets.get_loader import get_train_loader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_train_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LitModule(shape_edge_index=SHAPE_EMBEDDING_CFG.EDGE_INDEX,\n",
    "                shape_pose_n_features=SHAPE_EMBEDDING_CFG.POSE_N_FEATURES,\n",
    "                shape_n_hidden=SHAPE_EMBEDDING_CFG.N_HIDDEN,\n",
    "                shape_out_features=SHAPE_EMBEDDING_CFG.OUT_FEATURES,\n",
    "                shape_relation_layers=SHAPE_EMBEDDING_CFG.RELATION_LAYERS,\n",
    "                class_num=751,\n",
    "                r50_stride=FT_NET_CFG.R50_STRIDE,\n",
    "                r50_pretrained_weight=FT_NET_CFG.PRETRAINED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_logger = NeptuneLogger(\n",
    "    project=\"work_space/save/\",\n",
    "    api_key=\n",
    "    \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vbmV3LXVpLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2MWM3OWRkYi0yMTFlLTQzNjMtOGEzOS0yOGI0MjUxNmRkNjkifQ==\",\n",
    "    log_model_checkpoints=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type           | Params\n",
      "-----------------------------------------------------\n",
      "0 | ft_net            | FTNet          | 25.6 M\n",
      "1 | shape_embedding   | ShapeEmbedding | 300 K \n",
      "2 | fusion            | FusionNet      | 2     \n",
      "3 | id_classification | Linear         | 769 K \n",
      "-----------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "25.6 M    Non-trainable params\n",
      "26.6 M    Total params\n",
      "106.511   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5aae0f4f0154f7281d6f727a2aa7646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(accelerator='gpu', max_epochs=10)\n",
    "\n",
    "trainer.fit(model=net, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cuhk03/jsons/train.json', 'rb') as f:\n",
    "    img_list = json.load(f)\n",
    "\n",
    "import random  \n",
    "\n",
    "\n",
    "# def get_img_tensor(img_path):\n",
    "#     img = Image.open(img_path)\n",
    "#     img_tensor = transforms(img)\n",
    "#     return img_tensor\n",
    "\n",
    "\n",
    "# def get_pose_tensor(pose: List[List[float]]):\n",
    "#     return Tensor(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getitem(img_list):\n",
    "    failed = []\n",
    "    for a_img in img_list:\n",
    "        a_img_path = a_img['img_path']\n",
    "        a_id = a_img['p_id']\n",
    "        a_orientation = a_img['orientation']\n",
    "        a_pose = a_img['pose_landmarks']\n",
    "\n",
    "        same_id, diff_id = [], []\n",
    "        for item in img_list:\n",
    "            if item['p_id'] == a_id:\n",
    "                same_id.append(item)\n",
    "            else:\n",
    "                diff_id.append(item)\n",
    "\n",
    "        same_id_diff_ori, diff_id_same_ori = [], []\n",
    "\n",
    "        if 0 <= a_orientation <= 9 or 63 <= a_orientation <= 71 or 27 <= a_orientation <= 45:\n",
    "            # anchor has back or front orientation\n",
    "            for item in same_id:\n",
    "                if 45 <= item[\"orientation\"] < 63 or 9 <= item[\"orientation\"] < 27:\n",
    "                    # found positive sample of sideway orientation\n",
    "                    same_id_diff_ori.append(item)\n",
    "            for item in diff_id:\n",
    "                if 0 <= item[\"orientation\"] <= 9 or 63 <= item[\"orientation\"] <= 71 or 27 <= item[\"orientation\"] <= 45:\n",
    "                    diff_id_same_ori.append(item)\n",
    "        else:\n",
    "            # anchor has sideway orientation\n",
    "            for item in same_id:\n",
    "                if 0 <= item[\"orientation\"] <= 9 or 63 <= item[\"orientation\"] <= 71 or 27 <= item[\"orientation\"] <= 45:\n",
    "                    # found positive sample of back orientation\n",
    "                    same_id_diff_ori.append(item)\n",
    "            for item in diff_id:\n",
    "                if 45 <= item[\"orientation\"] < 63 or 9 <= item[\"orientation\"] < 27:\n",
    "                    diff_id_same_ori.append(item)\n",
    "\n",
    "        # if len(diff_id_same_ori) == 0:\n",
    "        #     print(\n",
    "        #         a_img_path\n",
    "        #     )\n",
    "    \n",
    "        try:\n",
    "            p_img = random.choice(same_id_diff_ori)\n",
    "            n_img = random.choice(diff_id_same_ori)\n",
    "        except: \n",
    "            failed.append(a_id)\n",
    "\n",
    "    return [*set(failed)]\n",
    "        # p_img_path = p_img['img_path']\n",
    "        # p_pose = p_img['pose_landmarks']\n",
    "\n",
    "        \n",
    "        # n_img_path = n_img['img_path']\n",
    "        # n_pose = n_img['pose_landmarks']\n",
    "\n",
    "        # a_img_tensor = get_img_tensor(a_img_path)\n",
    "        # p_img_tensor = get_img_tensor(p_img_path)\n",
    "        # n_img_tensor = get_img_tensor(n_img_path)\n",
    "        # a_pose_tensor = get_pose_tensor(a_pose)\n",
    "        # p_pose_tensor = get_pose_tensor(p_pose)\n",
    "        # n_pose_tensor = get_pose_tensor(n_pose)\n",
    "         #(a_img_tensor, p_img_tensor, n_img_tensor), (a_pose_tensor, p_pose_tensor, n_pose_tensor), a_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = getitem(img_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/dustin/DATA/Research/2DReID/SMPLMarket/bounding_box_train/0007/0007_c1s6_028546_01.jpg',\n",
       " '/media/dustin/DATA/Research/2DReID/SMPLMarket/bounding_box_train/0007/0007_c1s6_028546_04.jpg',\n",
       " '/media/dustin/DATA/Research/2DReID/SMPLMarket/bounding_box_train/0007/0007_c2s3_071002_01.jpg',\n",
       " '/media/dustin/DATA/Research/2DReID/SMPLMarket/bounding_box_train/0007/0007_c2s3_071052_01.jpg',\n",
       " '/media/dustin/DATA/Research/2DReID/SMPLMarket/bounding_box_train/0007/0007_c3s3_077419_03.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "failed[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
